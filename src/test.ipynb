{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "import timm\n",
    "\n",
    "\n",
    "weights = \"D:\\\\WorkFile\\\\Master\\\\Project\\\\Code\\\\Github\\\\finetuned_weights\\\\vit.pth\"\n",
    "checkpoint = torch.load(weights, map_location=\"cpu\")\n",
    "print(checkpoint.keys())\n",
    "\n",
    "model = timm.create_model('vit_base_patch8_224', pretrained=True)\n",
    "\n",
    "print(list(checkpoint.keys())[0].split(\".\")[0])\n",
    "\n",
    "for k in list(checkpoint):\n",
    "    tmp = \".\"\n",
    "    checkpoint[tmp.join(k.split(\".\")[1:])] = checkpoint.pop(k)\n",
    "\n",
    "print(checkpoint.keys())\n",
    "\n",
    "model.load_state_dict(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionTransformer(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(3, 768, kernel_size=(8, 8), stride=(8, 8))\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (norm_pre): Identity()\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (9): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (fc_norm): Identity()\n",
      "  (head): Linear(in_features=768, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "\n",
    "model = timm.create_model('vit_base_patch8_224', pretrained=True)\n",
    "model.blocks = model.blocks[:10]\n",
    "model.head = nn.Linear(in_features=model.head.in_features, out_features=3)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def draw_confusion_matrix(cm, classes, normalize=True, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    - cm : 计算出的混淆矩阵的值\n",
    "    - classes : 混淆矩阵中每一行每一列对应的列\n",
    "    - normalize : True:显示百分比, False:显示个数\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"显示百分比：\")\n",
    "        np.set_printoptions(formatter={'float': '{: 0.2f}'.format})\n",
    "        print(cm)\n",
    "    else:\n",
    "        print('显示具体数字：')\n",
    "        print(cm)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    #  matplotlib版本问题，如果不加下面这行代码，则绘制的混淆矩阵上下只能显示一半，有的版本的matplotlib不需要下面的代码，分别试一下即可\n",
    "    plt.ylim(len(classes) - 0.5, -0.5)\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "class ClassifyMetric:\n",
    "    # 这里的 labels 可以使用 None，这样就会让sklearn自己决定标签\n",
    "    def __init__(self, numClass, labels=None):\n",
    "        self.labels = labels\n",
    "        self.numClass = numClass\n",
    "        self.confusionMatrix = np.zeros((self.numClass,)*2)\n",
    "\n",
    "    def genConfusionMatrix(self, y_true, y_pred):\n",
    "        return confusion_matrix(y_true, y_pred, labels=self.labels)\n",
    "\n",
    "    def addBatch(self, y_true, y_pred):\n",
    "        assert np.array(y_true).shape == np.array(y_pred).shape\n",
    "        self.confusionMatrix += self.genConfusionMatrix(y_true, y_pred)\n",
    "\n",
    "    def reset(self):\n",
    "        self.confusionMatrix = np.zeros((self.numClass, self.numClass))\n",
    "\n",
    "    def accuracy(self):\n",
    "        accuracy = np.diag(self.confusionMatrix).sum() / \\\n",
    "            self.confusionMatrix.sum()\n",
    "        return accuracy\n",
    "\n",
    "    def precision(self):\n",
    "        precision = np.diag(self.confusionMatrix) / \\\n",
    "            self.confusionMatrix.sum(axis=0)\n",
    "        return np.nan_to_num(precision)\n",
    "\n",
    "    def recall(self):\n",
    "        recall = np.diag(self.confusionMatrix) / \\\n",
    "            self.confusionMatrix.sum(axis=1)\n",
    "        return recall\n",
    "\n",
    "    def f1_score(self):\n",
    "        precision = self.precision()\n",
    "        recall = self.recall()\n",
    "        f1_score = 2 * (precision*recall) / (precision+recall)\n",
    "        return np.nan_to_num(f1_score)\n",
    "\n",
    "metrics = ClassifyMetric(3, [\"LP\", \"LX\", \"RT\"])\n",
    "label       = ['LP', 'LP', 'LP', 'LP', 'LP', 'LP', 'LP', 'LP', 'LP', 'LX', 'LX', 'LX', 'LX', 'LX', 'LX', 'RT', 'RT', 'RT', 'RT', 'RT', 'RT', 'RT', 'RT', 'RT', 'RT', 'RT', 'RT', 'RT', 'RT', 'RT', 'RT', 'RT', 'RT', 'RT']\n",
    "prediction  = ['LP', 'LP', 'LP', 'LP', 'LP', 'LP', 'LP', 'LP', 'LP', 'LX', 'LX', 'LX', 'LX', 'LX', 'LX', 'LX', 'RT', 'RT', 'RT', 'LX', 'RT', 'RT', 'RT', 'RT', 'RT', 'RT', 'RT', 'RT', 'RT', 'RT', 'LX', 'RT', 'RT', 'LX']\n",
    "metrics.addBatch(label, prediction)\n",
    "matrix = metrics.genConfusionMatrix(label, prediction)\n",
    "draw_confusion_matrix(matrix, ['LP', 'LX', 'RT'],)\n",
    "\n",
    "accuracy = metrics.accuracy()\n",
    "precision = metrics.precision()\n",
    "recall = metrics.recall()\n",
    "f1_score = metrics.f1_score()\n",
    "\n",
    "print(40 * '*')\n",
    "print('accuracy:')\n",
    "print(accuracy)\n",
    "print('precision:')\n",
    "print(precision, end=\", Average: \")\n",
    "print(np.mean(precision))\n",
    "print('recall:')\n",
    "print(recall, end=\", Average: \")\n",
    "print(np.mean(recall))\n",
    "print('f1_score:')\n",
    "print(f1_score, end=\", Average: \")\n",
    "print(np.mean(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "FLOPS(G):\n",
      "51.90043759346008\n",
      "Params(MB)\n",
      "67.74023723602295\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "from components.se_sep_transformer import SeSepViT, FuSepViT\n",
    "from vit_pytorch.sep_vit import SepViT\n",
    "from components.senet.se_resnet import se_resnet18, se_resnet20, se_resnet34, se_resnet50, se_resnet56, se_resnet101, se_resnet152\n",
    "from components.fusion import fusion\n",
    "import timm\n",
    "\n",
    "from thop import profile\n",
    "\n",
    "input = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "# model = SeSepViT(                                                               \n",
    "#         num_classes = 1000,\n",
    "#         dim = 32,            \n",
    "#         dim_head = 32,   \n",
    "#         heads = (4, 8, 16, 32),\n",
    "#         depth = (1, 2, 14, 2),\n",
    "#         window_size = 7,      \n",
    "#         dropout = 0.1     \n",
    "#     )\n",
    "\n",
    "# model1 = FuSepViT(                                                               \n",
    "#             num_classes = 1000,\n",
    "#             dim = 32,           \n",
    "#             dim_head = 32,          \n",
    "#             heads = (1, 2, 4, 8),  \n",
    "#             depth = (1, 2, 6, 2),  \n",
    "#             window_size = 7,       \n",
    "#             dropout = 0.1          \n",
    "#         )\n",
    "# print(\"Loading pretrained weights...\")\n",
    "# checkpoint = torch.load(\"..\\\\pretrained_weights\\\\sepvit_lite.pth\", map_location=\"cpu\")[\"state_dict\"]\n",
    "# if list(checkpoint.keys())[0].split(\".\")[0] == \"module\":\n",
    "#     for k in list(checkpoint):\n",
    "#         tmp = \".\"\n",
    "#         checkpoint[tmp.join(k.split(\".\")[1:])] = checkpoint.pop(k)\n",
    "# checkpoint.pop(\"mlp_head.1.weight\")\n",
    "# checkpoint.pop(\"mlp_head.1.bias\")\n",
    "# checkpoint.pop(\"mlp_head.2.weight\")\n",
    "# checkpoint.pop(\"mlp_head.2.bias\")\n",
    "# model1.load_state_dict(checkpoint)\n",
    "# model2 = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "# layers = list(model2.children())[:-2]\n",
    "# model2 = torch.nn.Sequential(*layers)\n",
    "# model = fusion(model1, model2, 768, 3)\n",
    "# model = models.resnet18()\n",
    "model = timm.create_model('vit_base_patch8_224', pretrained=True)\n",
    "model.blocks = model.blocks[:10]\n",
    "model.head = nn.Linear(in_features=model.head.in_features, out_features=3)\n",
    "\n",
    "\n",
    "flops, params = profile(model, (input,))\n",
    "print(\"FLOPS(G):\")\n",
    "print(flops / (2 ** 30))\n",
    "print(\"Params(MB)\")\n",
    "print(params / (2 ** 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from itertools import cycle\n",
    "\n",
    "lg = logging.getLogger()\n",
    "lg.setLevel(\"INFO\") # 不想打印log时，把INFO替换成ERROR\n",
    "\n",
    "iris = load_iris()\n",
    "target_names = iris.target_names\n",
    "\n",
    "lg.info(\"=\" * 80)\n",
    "lg.info(\"target_names:\")\n",
    "lg.info(target_names)\n",
    "\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "lg.info(\"=\" * 80)\n",
    "lg.info(\"X:\")\n",
    "lg.info(X[:3])\n",
    "lg.info(\"X.shape:\")\n",
    "lg.info(X.shape)\n",
    "lg.info(\"=\" * 80)\n",
    "lg.info(\"y:\")\n",
    "lg.info(y[:3])\n",
    "\n",
    "y = iris.target_names[y]\n",
    "\n",
    "lg.info(\"=\" * 80)\n",
    "lg.info(y[:3])\n",
    "\n",
    "random_state = np.random.RandomState(0)\n",
    "n_samples, n_features = X.shape\n",
    "n_classes = len(np.unique(y))\n",
    "\n",
    "lg.info(\"=\" * 80)\n",
    "lg.info(\"n_samples:\")\n",
    "lg.info(n_samples)\n",
    "lg.info(\"=\" * 80)\n",
    "lg.info(\"n_features:\")\n",
    "lg.info(n_features)\n",
    "lg.info(\"=\" * 80)\n",
    "lg.info(\"n_classes:\")\n",
    "lg.info(n_classes)\n",
    "\n",
    "# 增长每个样本的特征向量，在每个特征向量后面拼接随机值\n",
    "X = np.concatenate([X, random_state.randn(n_samples, 200 * n_features)], axis=1)\n",
    "\n",
    "lg.info(\"=\" * 80)\n",
    "lg.info(\"X.shape after concatenate:\")\n",
    "lg.info(X.shape)\n",
    "\n",
    "(X_train, X_test, y_train, y_test,) = train_test_split(X, y, test_size=0.5, stratify=y, random_state=0)\n",
    "\n",
    "# 训练分类器\n",
    "classifier = LogisticRegression()\n",
    "y_score = classifier.fit(X_train, y_train).predict_proba(X_test)\n",
    "\n",
    "lg.info(\"=\" * 80)\n",
    "lg.info(\"y_score:\")\n",
    "lg.info(y_score[:3])\n",
    "\n",
    "# 制作one_hot标签\n",
    "label_binarizer = LabelBinarizer().fit(y_train)\n",
    "lg.info(\"=\" * 80)\n",
    "lg.info(\"y_test:\")\n",
    "lg.info(y_test[:3])\n",
    "y_onehot_test = label_binarizer.transform(y_test)\n",
    "\n",
    "# y_onehot_test = LabelBinarizer().fit_transform(y_test)\n",
    "\n",
    "lg.info(\"y_onehot_test:\")\n",
    "lg.info(y_onehot_test[:3])\n",
    "lg.info(\"y_onehot_test.shape:\")\n",
    "lg.info(y_onehot_test.shape)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "colors = cycle([\"aqua\", \"darkorange\", \"cornflowerblue\"])\n",
    "for class_id, color in zip(range(n_classes), colors):\n",
    "    RocCurveDisplay.from_predictions(\n",
    "        y_onehot_test[:, class_id],\n",
    "        y_score[:, class_id],\n",
    "        name=f\"ROC curve for {target_names[class_id]}\",\n",
    "        color=color,\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \"k--\", label=\"ROC curve for chance level (AUC = 0.5)\")\n",
    "plt.axis(\"square\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Extension of Receiver Operating Characteristic\\nto One-vs-Rest multiclass\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "x = np.arange(-10, 10, 0.001)\n",
    "a = x[x > 0]\n",
    "b = 0.1 * x[x < 0]\n",
    "y = np.concatenate((b, a), axis=0)\n",
    "plt.plot(x,y)\n",
    "plt.suptitle(r'$y=\\frac{1}{1+e^{-x}}$', fontsize=20)\n",
    "plt.grid(color='gray')\n",
    "plt.grid(linewidth='1')\n",
    "plt.grid(linestyle='--')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "import numpy as np \n",
    "\n",
    "one_hot_target = np.array([[0, 1], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1]])\n",
    "confidence = np.array([[0.27, 0.73], [0.64, 0.36], [0.15, 0.85], [0.97, 0.03], [0.42, 0.58], [0.89, 0.11], [0.21, 0.79], [0.32, 0.68], [0.47, 0.53], [0.81, 0.19]])\n",
    "idx_to_class = {0: '0', 1: '1'}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "colors = cycle([\"aqua\", \"darkorange\"])\n",
    "\n",
    "for class_id, color in zip(range(2), colors):\n",
    "    RocCurveDisplay.from_predictions(\n",
    "        one_hot_target[:, class_id],\n",
    "        confidence[:, class_id],\n",
    "        name=f\"ROC curve for {idx_to_class[class_id]}\",\n",
    "        color=color,\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "plt.axis(\"square\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Extension of Receiver Operating Characteristic\\nto One-vs-Rest multiclass\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from components.senet.se_resnet import se_resnet18\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from components.se_sep_transformer import FuSepViT\n",
    "\n",
    "\n",
    "x = torch.randn(16, 3, 224, 224)\n",
    "\n",
    "\n",
    "model = se_resnet18()\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "layers = list(model.children())[:-2]\n",
    "res = torch.nn.Sequential(*layers)\n",
    "print(res(x).shape)\n",
    "\n",
    "\n",
    "# model = FuSepViT(                                                               \n",
    "#             num_classes = 1000,\n",
    "#             dim = 32,           \n",
    "#             dim_head = 32,          \n",
    "#             heads = (1, 2, 4, 8),  \n",
    "#             depth = (1, 2, 6, 2),  \n",
    "#             window_size = 7,       \n",
    "#             dropout = 0.1          \n",
    "#         )\n",
    "# print(model(x).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from vit_pytorch.sep_vit import SepViT\n",
    "\n",
    "\n",
    "x = torch.randn(16, 3, 224, 224)\n",
    "\n",
    "model = model = SepViT(                                                               \n",
    "            num_classes = 1000,\n",
    "            dim = 32,           \n",
    "            dim_head = 32,          \n",
    "            heads = (1, 2, 4, 8),  \n",
    "            depth = (1, 2, 6, 2),  \n",
    "            window_size = 7,       \n",
    "            dropout = 0.1          \n",
    "        )\n",
    "\n",
    "checkpoint = torch.load(\"..\\\\pretrained_weights\\\\sepvit_lite.pth\", map_location=\"cpu\")[\"state_dict\"]\n",
    "if list(checkpoint.keys())[0].split(\".\")[0] == \"module\":\n",
    "            for k in list(checkpoint):\n",
    "                tmp = \".\"\n",
    "                checkpoint[tmp.join(k.split(\".\")[1:])] = checkpoint.pop(k)\n",
    "model.load_state_dict(checkpoint)\n",
    "checkpoint.pop(\"mlp_head.1.weight\")\n",
    "checkpoint.pop(\"mlp_head.1.bias\")\n",
    "checkpoint.pop(\"mlp_head.2.weight\")\n",
    "checkpoint.pop(\"mlp_head.2.bias\")\n",
    "print(checkpoint.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2469a70536e4d2335a2ea8907942d0699c37342a371ac185bdb5b0aa6f073890"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
